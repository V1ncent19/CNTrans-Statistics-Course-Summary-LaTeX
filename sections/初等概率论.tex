\section{概率论部分}\label{Section1Probability}
\begin{center}
    Instructor: Wanlu Deng
\end{center}

    Chapter Overview
\begin{itemize}[topsep=2pt,itemsep=2pt]
    \item \hyperlink{Basic axioms}{Basic axioms}
\end{itemize}

    









    Cover：Basic axioms, random events, $\sigma$-field; random variable/vector and their properties, some special distributions; $E$\,\&\,$\sigma^2$\,\&\,$cov$ and their properties; probability-generating/moment-generating/characteristic function; weak/strong law of large number, central limit thm.; intro. to multivariate normal distribution.



\subsection{Some Important Distributions}\label{SectionImportantDistributions}

\begin{table}[htbp]
    \centering
    \begin{tabular}{c|ccccc}
        \hline
        $X$&$p_X(k)//f_X(x)$&$\quad E\quad$&$\sigma^2$&PGF&MGF\\
        \hline
        $B(p)$& &$p$&$pq$&&$q+pe^s$\\
        $B(n,p)$&$C_n^k p^k(1-p)^{n-k}$&$np$&$npq$&$(q+ps)^n$&$(q+pe^s)^n$\\
        $G(p)$&$(1-p)^{k-1}p$&$\dfrac{1}{p}$&$\dfrac{q}{p^2}$&$\dfrac{ps}{1-qs}$&$\dfrac{pe^s}{1-qe^s}$\\
        $H(n,M,N)$&$\dfrac{C_M^kC_{N-M}^{n-k}}{C_N^n}$&$n\dfrac{M}{N}$&$\dfrac{nM(N-n)(N-M)}{N^2(n-1)}$&&\\
        $P(\lambda)$&$\dfrac{\lambda^k}{k!}e^{-\lambda}$&$\lambda$&$\lambda$&$e^{\lambda(s-1)}$&$e^{\lambda(e^s-1)}$\\
        $U(a,b)$&$\dfrac{1}{b-a}$&$\dfrac{a+b}{2}$&$\dfrac{(b-a)^2}{12}$&&$\dfrac{e^{sb}-e^{sa}}{(b-a)s}$\\
        $N(\mu,\sigma^2)$&$\dfrac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$&$\mu$&$\sigma^2$&&$e^{\frac{\sigma^2s^2}{2}+\mu s}$\\
        $\epsilon(\lambda)$&$\lambda e^{-\lambda x}$&$\dfrac{1}{\lambda}$&$\dfrac{1}{\lambda^2}$&&$\frac{\lambda}{\lambda-s}$\\
        $\Gamma(\alpha,\lambda)$&$\dfrac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}$&$\dfrac{\alpha}{\lambda}$&$\dfrac{\alpha}{\lambda^2}$&&\\
        $B(\alpha,\beta)$&$\dfrac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}$&$\dfrac{\alpha}{\alpha+\beta}$&$\dfrac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$&&\\
        $\chi^2_n$&$\dfrac{1}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}$&$n$&$2n$&&\\
        $t_\nu$&$\dfrac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}(1+\frac{x^2}{\nu})^{-\frac{\nu+1}{2}}$&$0$&$\dfrac{\nu}{\nu-2}$&&\\
        $F(m,n)$&$\dfrac{\Gamma(\frac{m+n}{2})}{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}\dfrac{m^\frac{m}{2}n^\frac{n}{2}x^{\frac{m}{2}-1}}{(mx+n)^{\frac{m+n}{2}}}$&$\dfrac{n}{n-2}$&$\dfrac{2n^2(m+n-2)}{m(n-2)^2(n-4)}$&&\\
        \hline
    \end{tabular}
\end{table}

    Definition of PGF, MGF, CF see section \hyperref[SectionPGFMGFCF]{\ref{SectionPGFMGFCF}}.

    More Properties of $\chi^2,t,F$ see section \hyperref[chi2_t_F_properties]{\ref{chi2_t_F_properties}}.



\subsection{Probability and Probability Model}

    What is \textbf{Probability}? 
    
    $\quad$ A 'belief' in the chance of an event occurring?


\subsubsection{Sample and $\sigma$-Field}
    Def. sample space $\Omega$: The set of \text{all} possible outcomes of one particular experiment.

    \index{$ \sigma $-field}Def. $\mathscr{F}$ a $\sigma$-field(or a $\sigma$-algebra) as a collection of some subsets of $\Omega$ \textbf{if}
    \begin{itemize}
        \item $\Omega\in\mathscr{F}$
        \item if $A\in\mathscr{F}$,then $A^C\in\mathscr{F}$
        \item if $A_n\in\mathscr{F}$, then ${\displaystyle\bigcup_{n=1}^\infty} A_n\in\mathscr{F}$
    \end{itemize}

    And $(\Omega,\mathscr{F})$ is a measurable space.
    

    
\subsubsection{Axioms of Probability}

    $P$ is probability measure (or probability function) defined on $(\Omega,\mathscr{F})$, satisfying

\begin{itemize}[itemsep=2pt,topsep=-2pt]
\item Nonnegativity
\begin{equation}    P(A)\geq 0\qquad \forall A\in\Omega    
\end{equation}
\item Normalization
\begin{equation}    P(\Omega)=1    
\end{equation}
\item Countable Additivity
\begin{equation}    P(A_1\cup A_2\cup\cdots)=P(A_1)+P(A_2)+\cdots\quad \, (A_i\parallel A_j\quad \forall i\neq j)
\end{equation}
\end{itemize}

    Then $(\Omega,\mathscr{F},P)$ is probability space\index{Probability Space}.

    Properties of Probability:
    \begin{itemize}
        \item Monotonicity
        \begin{equation}    
            P(A)\leq P(B)\quad \text{for}\, A\subset B
        \end{equation}
        \item Finite Subadditivity (Boole Inequality)
        \begin{equation}    
            P(\bigcup_{i=1}^nA_i)\leq\sum_{i=1}^n P(A_i)    
        \end{equation}
        \item Inclusion-Exclusion Formula\index{Inclusion-Exclusion Formula}
        \begin{align*}
            P(\Cup_{i=1}^nA_i)&=\sum_{1\leq i\leq n}P(A_i)-\sum_{1\leq i<j\leq n}P(A_i\cap A_j)\\
            &+\sum_{1\leq i<j<k\leq n}P(A_i\cap A_j\cap A_k)-\cdots\\
            &+(-1)^{n-1}P(A_1 \cap A_2\cap\cdots \cap A_n)
        \end{align*}
        \item Borel-Cantelli Lemma\index{Borel-Cantelli Lemma}
        \begin{gather*}
            \sum_{n=1}^\infty P(A_n)<\infty\Rightarrow P(\lim_{n\to\infty}\sup A_n)=0\\
            \sum_{n=1}^\infty P(A_n)=\infty\Rightarrow P(\lim_{n\to\infty}\sup A_n)=1\quad \text{if }A_i\text{ independent}
        \end{gather*}
    \end{itemize}


\subsubsection{Conditional Probability}
        Def. \textbf{Conditional Probability} of $B$ given $A$:
        \begin{equation}    
            P(B|A)=\frac{P(A\cap B)}{P(A)}    
        \end{equation}

        (Actually a change of $\sigma$-field from $\Omega$ to $B$)

        Application of conditional probability:
        \begin{itemize}
        \item Multiplication Formula
        \begin{equation}    
            P(\Cap_{i=1}^n A_i)=P(A_1)\prod_{i=2}^n P(A_i|A_1\cap A_2\cap \cdots\cap A_{i-1})    
        \end{equation}
        \item Total Probability Thm.
        \begin{equation}    
            P(B)=\sum_{i=1}^n P(A_i)P(B|A_i)  
        \end{equation}
        where $\{A_i\}$ is a partition of $\Omega$.
        \item Bayes's Rule
        \begin{equation}    
            P(A_i|B)=\frac{P(A_i)P(B|A_i)}{\sum_{j=1}^nP(A_j)P(B|A_j)}    
        \end{equation}
        where $\{A_i\}$ is a partition of $\Omega$.
        \item Statistically Independence
        \begin{equation}    
            P(A\cap B) =P(A)P(B),\text{ for }A\parallel B
        \end{equation}
    \end{itemize}

\subsection{Properties of Random Variable and Vector}\label{SectionPropertiesOfRandomVariableAndVector}

\subsubsection{Random Variable}
    Def. \text{Random Variable}: a \textbf{function} $X$ defined on sample space $\Omega$, mapping from $\Omega$ to some $\mathscr{X}\in\mathbb{R} $.

    Then def. Cumulative Distribution Function (CDF)\index{CDF (Cumulative Distribution Function)}.
    \begin{equation}
        F_X(x)=P(X\leq x)
    \end{equation}

    For Discrete case, consider CDF as right-continuity.

    \begin{itemize}
        \item

        \begin{center}
            \parbox[t]{8.65cm}{PMF:\index{PMF (Probability Mass Function)}\begin{equation}        p_X(x)=F_X(x^+)-F_X(x^-)\end{equation}}
            \parbox[t]{8.65cm}{PDF:\index{PDF (Probability Density Function)}
            \begin{equation}        
                f_X(x)=\frac{\mathrm{d}F_X(x)}{\mathrm{d}x}
            \end{equation}}
        \end{center}
        
        
        
        \item Indicator function:\index{Indicator Function}
        \begin{equation}    
            I_{x\in A}(x)=\begin{cases}
                1& x\in  A\\
                0& x\notin A
            \end{cases}
        \end{equation}
        \item Convolution
        \begin{itemize}
            \item $W=X+Y$
            \begin{equation}        
                f_W(w)=\int_{-\infty}^\infty f_X(x)f_Y(w-x)\mathrm{d}x    
            \end{equation}
            \item $V=X-Y$
            \begin{equation}        
                f_V(v)=\int_{-\infty}^\infty f_X(x)f_Y(x-v)\mathrm{d}x    
            \end{equation}
            \item $Z=XY$
            \begin{equation}        
                f_Z(z)=\int_{-\infty}^\infty \frac{1}{|x|}f_X(x)f_Y(\frac{z}{x})\mathrm{d}x
            \end{equation}
        \end{itemize}
        
        \item Order Statistics
        
        Def $X_{(1)},X_{(2)},\cdots,X_{(n)}$ as order statistics of $\vec{X}$
        \begin{equation}    
            g_{X_{(i)}}=n!\prod f(x_i)\qquad \mathrm{for}\, x_1<x_2\cdots <x_n    
        \end{equation}
        PDF of $X_{(k)}$
        \begin{equation}    
            g_k(x_k)=nC_{n-1}^{k-1}[F(x_k)]^{k-1}[1-F(x_k)]^{n-k}f(x_k)
        \end{equation}
        \item $p$-fractile\index{Fractile!$ p $-fractile}
        \begin{equation}    \xi_p=F^{-1}(p)=\inf\{x|F(x)\geq p\}\end{equation}
    \end{itemize}






\subsubsection{Random Vector}
    A general case of random variable.\index{r.v. (Random Variable or Random Vector)}

    $n$-dimension Random Vector $\vec{X}=(X_1,X_2,\ldots,X_n)$ defined on $(\Omega,\mathscr{F},P)$.

    CDF $F(x_1,\ldots,x_n)$ defined on $\mathbb{R}^n$:
    \begin{equation}F(x_1,\ldots,x_n)=P(X_1\leq x_1,\ldots,X_n\leq x_n)\end{equation}

    Joint PDF of random vector: 
    \begin{equation}
        f(x_1,\ldots,x_n)=\dfrac{\partial^n F(x_1,\ldots,x_n)}{\partial x_1\ldots\partial x_n}
    \end{equation}

    $k$-dimensional Marginal Distribution: For $1\leq k<n$ and index set $S_k=\{i_1.\ldots,i_k\}$, distribution of $\vec{X}=(X_{i_1},X_{i_2},\ldots,X_{i_k})$
    \begin{equation}F_{S_k}(x_{i_1},X_{i_2}\leq x_{i_2}\ldots,x_{i_k})=P(X_{i_1}\leq x_{i_1},\ldots,X_{i_k}\leq x_{i_k};X_{i_{k+1}},\ldots,X_{i_n}\leq\infty)\end{equation}

    Marginal distribution: 
    \begin{equation}
        g_{S_k}(x_{i_1},\ldots,x_{i_k})=\int_{\mathbb{R}^{n-k}}f(x_1,\ldots,x_n)\mathrm{d}x_{i_{k+1}}\ldots\mathrm{d}x_{j_n}=\dfrac{\partial^{n-k}F(x_1,\ldots,x_n)}{\partial x_{i_{k+1}}\ldots\partial x_{i_n}}
    \end{equation}


    \begin{itemize}
        \item[$\Delta$] \textbf{Function of r.v.}
        
        For $\vec{X}=(X_1,X_2,\cdots,X_n)$ with PDF $f(\vec{X})$ and define 
        \begin{equation}    
            \vec{Y}=(Y_1,Y_2,\cdots,Y_n)=(y_1(\vec{X}),y_2(\vec{X}),\cdots,y_n(\vec{X}))
        \end{equation}
        with inverse mapping
        \begin{equation}    
            \vec{X}=(X_1,X_2,\cdots,X_n)=(x_1(\vec{Y}),x_2(\vec{Y}),\cdots,x_n(\vec{Y}))
        \end{equation}
        then
        \begin{equation}    
            g(\vec{Y})= f(x_1(\vec{Y}),x_2(\vec{Y}),\cdots,x_n(\vec{Y}))\left|\frac{\partial \vec{X}}{\partial\vec{Y}}\right|I_{D_Y}
        \end{equation}

        (Intuitively: $g(\vec{Y})\mathrm{d}\vec{Y}=\mathrm{d}P=f(\vec{X})\mathrm{d}\vec{X}$)
    \end{itemize}




\subsection{Properties of $E$ , $\mathbf{\sigma^2}$ and $cov$}

    Expectation and Variance of common distributions see sec.\ref{SectionImportantDistributions}.

\subsubsection{Expection}
    Expectation of r.v. $g(X)$ def.:
    \begin{equation}
    E[g(X)]=\begin{cases}
        {\displaystyle\int_\Omega g(x) f_X(x)\mathrm{d}x=\int_\Omega g(x)\mathrm{d}F(x)}\\
        {\displaystyle\sum_{\Omega}g(X)f_X(x)}
    \end{cases}
\end{equation}

    Properties of expectation $E(\cdot)$:
\begin{itemize}
    \item Linearity of Expectation\begin{equation}
        E(aX+bY)=aE(X)+bE(Y)
    \end{equation}
    \item Conditional Expectation\begin{equation}
        E(X|A)=\frac{E(XI_A)}{P(A)}
    \end{equation}
    
    Note: if take $A$ as $Y$ is also a r.v. then 
    \begin{equation}m(Y)=E(X|Y)=\int xf_{X|Y}(x)\mathrm{d}x\end{equation}

    is actually a function of $Y$

    \item Law of Total Expectation\begin{equation}
    E\{E[g(X)|Y]\}=E[g(X)]
    \end{equation}
    \item r.v.\& Event
    \begin{equation}
        P(A|X)=E(I_A|X)\Rightarrow E[P(A|X)]=E(I_A)=P(A)
    \end{equation}
    \item \begin{equation}
        E[h(Y)g(X)|Y]=h(Y)E[g(X)|Y]
    \end{equation}
\end{itemize}


\subsubsection{Variance}
    Variance of r.v. $X$: 
    \begin{equation}
        var(X)=E[(X-E(X))^2]=E(X^2)-(E(X))^2
    \end{equation}

    (sometimes denoted as $\sigma^2_X$.)

    Properties:
\begin{itemize} 
    \item Linear combination of Variance\begin{equation}
        var(aX+b)=a^2var(X)
    \end{equation}
    \item Conditional Variance
    \begin{equation}
        var(X|Y)=E{[X-E(X|Y)]^2|Y}
    \end{equation}
    \item Law of Total Variance\begin{equation}
        var(X)=E[var(X|Y)]+var[E(X|Y)]
    \end{equation}
\end{itemize}

    Standard Deviation def. as :
    \begin{equation}\sigma_X=\sqrt{var(X)}\end{equation}

    Then can construct \textbf{Standardization}\index{Standardization} of r.v.
    \begin{equation}Y=\frac{X-E(X)}{\sqrt{var(X)}}\end{equation}


\subsubsection{Covariance and Correlation}\label{SubSubSectionCovarianceAndCorrelation}
    Covariance of r.v. $X$ and $Y$:\begin{equation}
        cov(X,Y)=E[(X-\mu_X)(Y-\mu_Y)]=E(XY)-E(X)E(Y)
    \end{equation}

    And (Pearson's) Correlation Coefficient\index{Pearson's Correlation Coefficient}\begin{equation}
        \rho_{X,Y}=corr(X,Y)=\frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}
    \end{equation}

    Remark: correlation $\nRightarrow$ cause and effect.

    Properties:
\begin{itemize}
\item Bilinear of Covariance\begin{align*}
    cov(X+Y,Z)&=cov(X,Z)+cov(Y,Z)\\
    cov(X,Y+Z)&=cov(X,Y)+cov(X,Z)
\end{align*}
    
\item Variance and Covariance\begin{equation}label{EqaVarOfSumOfRV}
    var(X+Y)=var(X)+var(Y)+2cov(X,Y)
\end{equation}
\item Covariance Matrix

    Def $\Sigma=E[(X-\mu)(X-\mu)^T]=\{\sigma_{ij}\}$ (where $X$ should be considered as a column vector)
\begin{equation}\label{covariancematrix}
    \Sigma=
        \begin{pmatrix}
        var(X_1) & cov(X_1,X_2) & \ldots & cov(X_1,X_n)\\
        cov(X_2,X_1) & var(X_2) & \ldots & cov(X_2,X_n)\\
        \vdots & \vdots & \ddots & \vdots\\
        cov(X_n,X_1) & cov(X_n,X_2) & \ldots & var(X_n)\\
        \end{pmatrix}    
    \end{equation}
\end{itemize}

Attachment: Independence:\begin{equation}    X_i || X_j\Rightarrow \begin{cases}
        f(x_1,x_2,\cdots,x_n)=\prod f(x_i)\\
        F(x_1,x_2,\cdots,x_n)=\prod F(x_i)\\
        E(\prod X_i)=\prod E(X_i)\\
        var(\sum X_i)=\sum var(X_i)
    \end{cases}
\end{equation}


\subsection{PGF, MGF and C.F}\label{SectionPGFMGFCF}

    Generating Function: Representation of $P$ in function space. $P\Leftrightarrow$ Generating Function.

\subsubsection{Probability Generating Function}
    PGF\index{PGF (Probability Generating Function)}: used for non-negative, integer $X$
    \begin{equation}
        g(s)=E(s^X)=\sum_{j=0}^\infty s^jP(X=j)    ,s\in[-1,1]
    \end{equation}

    Properties
    \begin{itemize}
        \item $P(X=k)=\dfrac{g^{(k)}(0)}{k!}$
        \item $E(X)=g^{(1)}(1)$
        \item $var(X)=g^{(2)}(1)+g^{(1)}(1)-[g^{(1)}(1)]^2 $
        \item For $X_1,X_2,\cdots,X_n$ independent with $g_i(s)=E(s^{X_i})$, $Y={\displaystyle \sum_{i=1}^n} X_i$, then
        \begin{equation}    
            g_Y(s)=\prod_{i=1}^n g_i(s),s\in[-1,1]
        \end{equation}
        \item For ${X_i}$ i.i.d with $\psi(s)=E(s^{X_i})$, $Y$ with $G(s)=E(s^{Y})$, $W=X_1+X_2+\cdots +X_Y$,then
        \begin{equation}    
            g_W(s)=G[\psi(s)]    
        \end{equation}
        \item 2-Dimensional PGF of $(X,Y)$
        \begin{equation}    
            g(s,t)=E(s^Xt^Y)=\sum_{i=o}^\infty\sum_{j=0}^\infty P_{(X,Y)}(X=i,Y=j)s^it^j,\, s,t\in[-1,1]
        \end{equation}
    \end{itemize}
\subsubsection{Moment Generating Function}
    MGF\index{MGF (Moment Generating Function)}: 
    \begin{equation}
        M_X(s)=E(e^{sX})=\begin{cases}
            \sum_je^{sx}P(X=x_j)\\
            \int_{-\infty}^\infty e^{sx}f_X(x)\mathrm{d}x
        \end{cases}
    \end{equation}

    Properties
    \begin{itemize}
        \item MGF of $Y=aX+b$: $
            M_Y(s)=e^{sb}M(sa)    $
        \item $E(X^k)=M^{(k)}(0)$
        \item $P(X=0)={\displaystyle\lim_{s\to -\infty}}M(s)$
        \item For $X_1,X_2,\cdots,X_n$ independent with $M_{X_i}(s)=E(e^{sX_i})$, $Y={\displaystyle \sum_{i=1}^n} X_i$, then
        \begin{equation}    
            M_Y(s)=\prod_{i=1}^n M_{X_i}(s)
        \end{equation}
    \end{itemize}
\subsubsection{Characteristic Function}
    C.F \index{C.F. (Characteristic Function)}is actually the Fourier Transform of $f$.
    \begin{equation}
        \phi(t)=E(e^{itX}) = \int_{-\infty}^\infty e^{itx}f_X(x)\mathrm{d}x
    \end{equation}

    Properties
    \begin{itemize}
    \item if $E(|X|^k)<\infty$,then
    \begin{equation}
        \phi^{(k)}(t)=i^kE(X^ke^{itX})\qquad \phi^{(k)}(0)=i^kE(X^k)    
    \end{equation}
    \item For $X_1,X_2,\cdots,X_n$ independent with $\phi_{X_i}(t)=E(e^{itX_i})$, $Y={\displaystyle \sum_{i=1}^n} X_i$, then
    \begin{equation}
        \phi_Y(t)=\prod_{i=1}^n \phi_{X_i}(t)
    \end{equation}
    \item Inverse (Fourier) Transform
    \begin{equation}
        f(x)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{-itx}\phi(t)\mathrm{d}t    
    \end{equation}
\end{itemize}



\subsection{Convergence and Limit Distribution}
\subsubsection{Convergence Mode}
    \index{Convergence}
    \begin{equation}
        \begin{cases}
            \text{Convergence in Distribution }&{\displaystyle X_n\xrightarrow[]{\mathscr{L}}X:\lim_{n\to\infty}F_n(x)=F(x)}\\
            \text{Convergence in Probability }&{\displaystyle X_n\xrightarrow[]{p}X:\lim_{n\to\infty}P(|X_n-X)\geq\varepsilon)=0\, ,\forall\varepsilon>0}\\
            \text{Almost Sure Convergence }&{\displaystyle X_n\xrightarrow[]{\text{a.s.}}X:P(\lim_{n\to\infty}X_n=X)=1}\\
            L_p\text{ Convergence }&{\displaystyle X_n\xrightarrow[]{L_p}X:\lim_{n\to\infty}E(|X_n-X|^p)=0}
        \end{cases}
    \end{equation}

        Relations between convergence:
        \begin{center}
            \begin{tikzpicture}
                \draw(-1,-1)rectangle(1,1);
                \draw(-3,-0.5)rectangle(-5,-2.5);
                \draw(-3,0.5)rectangle(-5,2.5);
                \draw(3,-1)rectangle(5,1);
                \draw[-latex](-3,-1.5)--(-1,-0.5);
                \draw[-latex](-3,1.5)--(-1,0.5);
                \draw[-latex](1,0)--(3,0);
                \node at (0,0){$X_n\xrightarrow[]{p}X$};
                \node at (-4,-1.5){$X_n\xrightarrow[]{L_p}X$};
                \node at (-4,1.5){$X_n\xrightarrow[]{\text{a.s.}}X$};
                \node at (4,0){$X_n\xrightarrow[]{\mathscr{L}}X$};
            \end{tikzpicture}
        \end{center}

        Useful Thm.:
        \begin{itemize}
            \item Continuous Mapping Thm.: For continuous function $g(\cdot)$
            \begin{enumerate}
                \item $X_n\xrightarrow[]{\text{a.s.}}X\Rightarrow g(X_n)\xrightarrow[]{\text{a.s.}}g(X)$
                \item $X_n\xrightarrow[]{p}X\Rightarrow g(X_n)\xrightarrow[]{p}g(X)$
                \item $X_n\xrightarrow[]{\mathscr{L}}X\Rightarrow g(X_n)\xrightarrow[]{\mathscr{L}}g(X)$
            \end{enumerate}
            \item Slutsky's Thm.\index{Slutsky's Thm.}: For $X_n\xrightarrow[]{\mathscr{L}}X,Y_n\xrightarrow[]{p}c$
            \begin{enumerate}
                \item $X_n+Y_n\xrightarrow[]{\mathscr{L}}X+c$
                \item $X_nY_n\xrightarrow[]{\mathscr{L}}cX$
                \item $X_n/Y_n\xrightarrow[]{\mathscr{L}}X/c$
            \end{enumerate}
            \item Continuity Thm.
            \begin{equation}        \lim_{n\to\infty}\phi_n(t)=\varphi(t)\Leftrightarrow X_n\xrightarrow[]{\mathscr{L}}X\end{equation}
        \end{itemize} 


\subsubsection{Law of Large Number \& Central Limit Theorem}

\begin{itemize}
    \item WLLN\index{LLN (Law of Large Number)}
\begin{equation}    \frac{1}{n}\sum X_i\xrightarrow[]{p} E(X_1)
\end{equation}
\item SLLN
\begin{equation}    \frac{1}{n}\sum X_i\xrightarrow[]{\text{a.s.}}  C
\end{equation}
\item CLT\index{CLT (Central Limit Theorem)}
\begin{equation}    \frac{1}{\sigma\sqrt{n}}\sum(X_k-\mu)\xrightarrow[]{\mathscr{L}} N(0,1)
\end{equation}
\item de Moivre-Laplace Thm.
\begin{equation}    P(k\leq S_n\leq m)\approx \Phi(\frac{m+0.5-np}{\sqrt{npq}})-\Phi(\frac{k-0.5-np}{\sqrt{npq}})
\end{equation}
\item Stirling Eqa
\begin{equation}    \frac{\lambda^k}{k!}e^{-\lambda}\approx \frac{1}{\sqrt{\lambda}\sqrt{2\pi}}e^{-\frac{(k-\lambda)^2}{2\lambda}}\xrightarrow[\lambda=n]{k=n}n!\approx\sqrt{2\pi n}(\frac{n}{e})^n
\end{equation}

\end{itemize}


\subsection{Inequalities}
    
\begin{itemize}
    \item Cauchy-Schwarz Inequality\index{Inequality!Cauchy-Schwarz Inequality}
    \begin{equation}
        |E(XY)|\leq\sqrt{E(X^2)E(Y^2)}
    \end{equation}

    \item Bonferroni Inequality\index{Inequality!Bonferroni Inequality}
\begin{equation}    P(\bigcup_{i=1}^n A_i)\geq \sum_{1\leq i\leq n} P(A_i)+\sum_{1\leq i <j\leq n} P(A_i\cap A_j)
\end{equation}
    \item Markov Inequality\index{Inequality!Markov Inequality}
\begin{equation}    P(|X|\geq \epsilon)\leq\frac{E(|X|^\alpha)}{\epsilon^\alpha}
\end{equation}

    \item Chebyshev Inequality\index{Inequality!Chebyshev Inequality}
\begin{equation}    P(|X-E(X)|\geq\epsilon)\leq\frac{var(X)}{\epsilon^2}
\end{equation}
    \item Jensen Inequality: For convex function $g(x)$:
\begin{equation}    E[g(X)]\geq g(E(X))
\end{equation}

\end{itemize}


\subsection{Multivariate Normal Distribution}\label{SubsectionDerivationMultivariateNormal}
    For $X_1,X_2,\cdots,X_n$ independent and $X_k\sim N(\mu_k,\sigma^2_k),\, k=1,\cdots,n$, $T={\displaystyle\sum_{k=1}^n c_kX_k}, (c_k$ const), then
    \begin{equation}
        T\sim N(\sum_{k=1}^nc_k\mu_k,\sum_{k=1}^n c_k^2\sigma^2_k)    
    \end{equation}

    Deduction in some special cases:
    \begin{itemize}
        \item Given $\mu_1=\mu_2=\cdots=\mu_n=\mu,\, \sigma^2_1=\sigma^2_2=\cdots=\sigma^2_n=\sigma^2$, i.e. $X_k$ i.i.d., then
        \begin{equation}\label{EqaDistributionOfSumOfiidNormal}
            T\sim   N(\mu\sum_{k=1}^n c_k,\sigma^2\sum_{k=1}^n c_k^2) 
        \end{equation}
        \item Further take $c_1=c_2=\cdots=c_n=\dfrac{1}{n}$, i.e. $T={\displaystyle \sum_{k=1}^n X_k /n}=\bar{X}$, then
        \begin{equation}    
            T=\bar{X}\sim N(\mu,\frac{\sigma^2}{n})    
        \end{equation}
    \end{itemize}





\subsubsection{Linear Transform}
    First consider $\epsilon_1,\epsilon_2,\cdots,\epsilon_m$ i.i.d. $\sim N(0,1)$, $n\times 1$ const column vector $\vec{\mu}$, $n\times m$ const matrix $\mathbf{B}=\{b_{ij}\}$, def.$X_i={\displaystyle\sum_{j=1}^m b_{ij}\epsilon_j}$, i.e.
    \begin{equation}
        \vec{X}=
        \begin{pmatrix}
            X_1\\X_2\\ \vdots\\X_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            b_{11}&b_{12}&\ldots&b_{1m}\\
            b_{21}&b_{22}&\ldots&b_{2m}\\
            \vdots&\vdots&\ddots&\vdots\\
            b_{n1}&b_{n2}&\ldots&b_{nm}
        \end{pmatrix}
        \begin{pmatrix}
            \epsilon_1\\
            \epsilon_2\\
            \vdots\\
            \epsilon_m
        \end{pmatrix}
        +\vec{\mu}
    \end{equation}

    
    We have: $\vec{X}\sim N(\vec{\mu},\Sigma)$, where $\Sigma$, as defined in eqa.\ref{covariancematrix} is
    \begin{equation}
        \Sigma=E[(\vec{X}-\vec{\mu})(\vec{X}-\vec{\mu})^T]=\mathbf{BB}^T=
        \begin{pmatrix}
        var(X_1) & cov(X_1,X_2) & \ldots & cov(X_1,X_n)\\
        cov(X_2,X_1) & var(X_2) & \ldots & cov(X_2,X_n)\\
        \vdots & \vdots & \ddots & \vdots\\
        cov(X_n,X_1) & cov(X_n,X_2) & \ldots & var(X_n)\\
        \end{pmatrix}  
        =\{\sigma_{ij}\}  
    \end{equation}

    Furthur Consider $\vec{Y}=(Y_1,\cdots,Y_n)^T$, $n\times n$ const square matrix $\mathbf{A}=\{a_{ij}\}$ and def. $\vec{Y}=\mathbf{A}\vec{X}$ i.e.
    \begin{equation}
        \begin{pmatrix}
            Y_1\\
            Y_2\\
            \vdots\\
            Y_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            a_{11}&a_{12}&\ldots&a_{1n}\\
            a_{21}&a_{22}&\ldots&a_{2n}\\
            \vdots&\vdots&\ddots&\vdots\\
            a_{n1}&a_{n2}&\ldots&a_{nn}
        \end{pmatrix}
        \begin{pmatrix}
            X_1\\
            X_2\\
            \vdots\\
            X_n
        \end{pmatrix}
    \end{equation}

    Then $\vec{Y}\sim N(\mathbf{A}\vec{\mu},\mathbf{A}\Sigma\mathbf{A}^T)$
%    Then $Y_1,\cdots,Y_n\sim N$ 

    Special case: $X_1,\cdots,X_n$ i.i.d. $\sim N(\mu,\sigma^2)$, $\vec{X}=(X_1,\cdots,X_n)^T$, 
    \begin{align*}
        E(Y_i)=&\mu\sum_{k=1}^n a_{ik}\\
        var(Y_i)=&\sigma^2\sum_{k=1}^n a_{ik}^2\\
        cov(Y_i,Y_j)=&\sigma^2\sum_{k=1}^n a_{ik} a_{jk}
    \end{align*}

    Specially when 
    $\mathbf{A}=\{a_{ij}\}$ orthonormal, we have $Y_1,\cdots,Y_n$ independent
    \begin{equation}
        Y_i\sim N(\mu\sum_{k=1}^n a_{ik},\sigma^2)    
    \end{equation}

    \subsubsection{Distributions of Function of Normal Variable: $\chi^2,$ $t\,\& \,F$}\label{chi2_t_F_properties}
        Consider $X_1,X_2,\ldots,X_n$ i.i.d. $\sim N(0,1)$; $Y,Y_1,Y_2,\ldots,Y_m$ i.i.d. $\sim N(0,1)$
        \begin{itemize}
            \item $\chi^2$ Distribution: Def. $\chi^2$ distribution with degree of freedom $n$:
            \begin{equation}        
                \xi =\sum_{i=1}^n X_i^2\sim \chi^2_n
            \end{equation}

            PDF of $\chi^2_n$:
            \begin{equation}        
                g_n(x)=\dfrac{1}{2^{n/2}\Gamma(n/2)}x^{n/2}e^{-x/2}I_{x>0}  
            \end{equation}

            Properties
            \begin{itemize}
                \item $E$ and $var$ of $\xi\sim\chi^2_n$
                \begin{equation}            E(\xi)=n\qquad var(\xi)=2n\end{equation}
                \item For independent $\xi_i\sim\chi^2_{n_i},\, i=1,2,\ldots,k$:\begin{equation}            
                    \xi_0=\sum_{i=1}^k\xi_i\sim\chi^2_{n_1+\ldots+n_k}\end{equation}
                \item Denoted as $\Gamma(\alpha,\lambda)$: \begin{equation}            \xi=\sum_{i=1}^nX_i\sim\Gamma(\frac{n}{2},\frac{1}{2})=\chi^2_n\end{equation}
            \end{itemize}
            \item $t$ Distribution: Def. $t$ distribution with degree of freedom $n$:
            \begin{equation}        
                T=\frac{Y}{\sqrt{\dfrac{\sum_{i=1}^nX_i^2}{n}}}=\frac{Y}{\sqrt{\dfrac{\xi}{n}}}\sim t_n
            \end{equation}

            (Usually take $\nu$ instead of $n$)

            PDF of $t_\nu$:
            \begin{equation}        t_\nu(x)=\dfrac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}\left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}\end{equation}

            Denote: Upper $\alpha$-fractile\index{Fractile!Upper $ \alpha $-fractile} of $t_\nu$, satisfies $P(T\geq c)=\alpha$:
            \begin{equation}        
                c=t_{\nu,\alpha}
            \end{equation}
            
            (Similar for $\chi^2_n$ and $F_{m,n}$ etc.)
            \item $F$ Distribution: Def. $F$ distribution with degree of freedom $m$ and $n$:
            \begin{equation}        
                F=\frac{\sum_{i=1}^mY_i}{\sum_{i=1}^nX_i}\sim F_{m,n}
            \end{equation}

            PDF of $F_{m,n}$:
            \begin{equation}        
                f_{m,n}(x)=\frac{\Gamma(\frac{m+n}{2})m^\frac{m}{2}n^{\frac{n}{2}}}{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}x^{\frac{m}{2}-1}(mx+n)^{-\frac{m+n}{2}} I_{x>0}
            \end{equation}

            Properties
            \begin{itemize}
                \item If $Z\sim F_{m,n}$, then $\dfrac{1}{Z}\sim F_{n,m}$.
                \item If $T\sim t_n$, then $T^2\sim F_{1,n}$
                \item $F_{m,n,1-\alpha}=\dfrac{1}{F_{n,m,\alpha}}$
            \end{itemize}
        \end{itemize}

        \begin{point}
            Some useful Lemma (uesd in statistic inference, see section \ref{SubSectionConfidenceIntervalForDistributions}):
        \end{point}
        
            
        \begin{itemize}
            \item For $X_1,X_2,\ldots,X_n$ independent with $X_i\sim N(\mu_i,\sigma^2_i)$, then
            \begin{equation}        
                \sum_{i=1}^n\left(\frac{X_i-\mu_i}{\sigma_i}\right)^2\sim \chi^2_n
            \end{equation}  
            \item For $X_1,X_2,\ldots,X_n$ i.i.d.$\sim N(\mu,\sigma^2)$, then
            \begin{equation}        
                T=\frac{\sqrt{n}(\bar{X}-\mu)}{S}\sim t_{n-1}   
            \end{equation}
            
            For $X_1,X_2,\ldots,X_m$ i.i.d.$\sim N(\mu_1,\sigma^2)$, $Y_1,Y_2,\ldots,Y_n$ i.i.d.$\sim N(\mu_2,\sigma^2)$, \\ denote sample pooled variance $S_{\omega}^2=\dfrac{(m-1)S^2_1+(n-1)S^2_2}{m+n-2}$, then
            \begin{equation}        
                T=\frac{(\bar{X}-\bar{Y})-(\mu_1-\mu_2)}{S_{\omega}}\cdot \sqrt{\frac{mn}{m+n}}\sim t_{m+n-2}
            \end{equation}
            \item For $X_1,X_2,\ldots,X_m$ i.i.d.$\sim N(\mu,\sigma^2)$, $Y_1,Y_2,\ldots,Y_n$ i.i.d.$\sim N(\mu_2,\sigma^2)$, then
            \begin{equation}        
                T=\frac{S_1^2}{S_2^2}\frac{\sigma^2_2}{\sigma^2_1}\sim F_{m-1,n-1}   
            \end{equation}
            \item For $X_1,X_2,\ldots,X_n$ i.i.d. $\sim \epsilon(\lambda)$, then
            \begin{equation}        
                2\lambda n\bar{X}=2\lambda\sum_{i=1}^nX_i \sim\chi^2_{2n} 
            \end{equation}

            Remark: for $X_i\sim\epsilon(\lambda)=\Gamma(1,\lambda)\Rightarrow 2\lambda\sum_{i=1}^nX_i\sim\Gamma(n,1/2)=\chi^2_{2n}$. 
        \end{itemize}

